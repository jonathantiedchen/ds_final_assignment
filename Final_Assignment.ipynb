{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Document Similarity\n",
    "Create a python program that will compute the text document similarity between different docu- ments. Your implementation will take a list of documents as an input text corpus, and it will compute a dictionary of words for the given corpus. Later, when a new document (i.e, search document) is provided, your implementation should provide a list of documents that are similar to the given search document, in descending order of their similarity with the search document.\n",
    "For computing similarity between any two documents in our question, you can use the following distance measures (optionally, you can also use any other measure as well).\n",
    "1. dot product between the two vectors\n",
    "2. distance norm (or Euclidean distance) between two vectors .e.g. || u − v ||\n",
    "\n",
    "As part of answering the question, you can also compare and comment on which of the two methods (or any other measure if you have used some other measure) will perform better and what are the reasons for it.\n",
    "\n",
    "Hint A text document can be represented as a word vector against a given dictionary of words. So first, compute the dictionary of words for a given text corpus containing the unique words from the documents of the given corpus. Then transform every text document of the given corpus into vector form, i.e., creating a word vector where 0 indicates the word is not in the document, and 1 indicates that the word is present in the given document. In our question, a text document is just represented as a string, so the text corpus is nothing but a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "**def_input_corpus**: \n",
    "- input text corpus containing of multiple strings CHECK \n",
    "- words from corpus to dictionary CHECK\n",
    "- split inpt strings into words (account for .:,;!&/()=\"\")) CHECK\n",
    "- append words to a library (list of distinct words) CHECK\n",
    "- create vektors fo text documents CHECK\n",
    "\n",
    "\n",
    "\n",
    "**def_input_new_document**:\n",
    "- create vektor for text document based on corpus CHECK\n",
    "- calculate difference between input text and every existing text document CHECK\n",
    "- create a list of the documents and their similarity rating -> dictionary used -> probably convert it to df in the end\n",
    "\n",
    "\n",
    "**things to add**:\n",
    "- document names\n",
    "- reading of documents from drive path (.txt file) \n",
    "- word frequency for documents -> not binary vector anymore\n",
    "- change dictionary to set instead of list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Jonathan \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "dictionary = set()\n",
    "corpus = []\n",
    "name_list = []\n",
    "\n",
    "def process_text(text):\n",
    "  # extract words from string and return in lower case\n",
    "  return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def create_dictionary(corpus):\n",
    "  global dictionary\n",
    "  \n",
    "  #iterate through all documents from the corpus\n",
    "  for document in corpus:  \n",
    "   \n",
    "    #extract all words in lower case from the string\n",
    "    #save words as a set and add words to the dictionary by building the union\n",
    "    words = process_text(document)\n",
    "    words_doc = set(words)\n",
    "    dictionary = dictionary.union(words_doc)\n",
    "    \n",
    "    ################### old\n",
    "    #iterate thorugh the words of each document and append to diciontary when not already in dictionary\n",
    "    ## instead of loop, do set(words) and then union this set with the dictionary set\n",
    "    #for word in words:\n",
    "    #  if word not in dictionary:\n",
    "    #    dictionary.append(word)\n",
    "  #dictionary = sorted(dictionary)\n",
    "    ##############################\n",
    "  return dictionary \n",
    "\n",
    "def document_to_vector(document, dictionary):\n",
    "  # Convert a document into a binary word vector\n",
    "  word_list = process_text(document)\n",
    "  doc_vector = [1 if word in word_list else 0 for word in dictionary]\n",
    "  return np.array(doc_vector)\n",
    "\n",
    "\n",
    "\n",
    "####### dot product #################################\n",
    "\n",
    "def compute_dot_similarity(doc_vectors, search_vector, corpus):\n",
    "    similarity_dic = {}\n",
    "    #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "    for i, doc_vector in enumerate(doc_vectors):\n",
    "        similarity = np.dot(doc_vector, search_vector)\n",
    "        #similarity=np.dot(doc, search_vector)/len(process_text(corpus[i]))\n",
    "        doc_name = corpus[i]\n",
    "        #doc_name = word_list[i]\n",
    "        similarity_dic.update({doc_name: similarity})\n",
    "    return similarity_dic\n",
    "\n",
    "def similarity_dot_prod(search_doc,corpus):\n",
    "  dictionary = create_dictionary(corpus)\n",
    "  doc_vectors = np.array([document_to_vector(document, dictionary) for document in corpus])\n",
    "  #print(\"doc vectors: \", doc_vectors)\n",
    "  #print(\"dicdic\",dictionary)\n",
    "  search_vector = document_to_vector(search_doc, dictionary)\n",
    "  #print(\"search vector:\", search_vector)\n",
    "  similarities = compute_dot_similarity(doc_vectors,search_vector,corpus)\n",
    "  \n",
    "  #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "  similarities = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "  similarities.sort_values([\"Similarity\"], ascending=False)\n",
    "  return similarities\n",
    "\n",
    "\n",
    "\n",
    "####### jaccard index #################################\n",
    "\n",
    "def compute_jac_similarity(doc_vectors, search_vector, corpus):\n",
    "    similarity_dic = {}\n",
    "    #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "    for i, doc_vector in enumerate(doc_vectors):\n",
    "        cor_len= len(process_text(corpus[i]))\n",
    "        search_len = len(process_text(str(search_vector)))\n",
    "        \n",
    "        #divide the dot product by the number of words in the union of search_doc and doc from corpus\n",
    "        similarity=np.dot(doc_vector, search_vector)/(search_len+cor_len)\n",
    "        \n",
    "        #print(len(process_text(corpus[i])))\n",
    "        doc_name = corpus[i]\n",
    "        similarity_dic.update({doc_name: similarity})\n",
    "    \n",
    "    return similarity_dic\n",
    "\n",
    "def similarity_jac_ind(search_doc,corpus):\n",
    "  dictionary = create_dictionary(corpus)\n",
    "  doc_vectors = np.array([document_to_vector(document, dictionary) for document in corpus])\n",
    "  #print(\"doc vectors: \", doc_vectors)\n",
    "  #print(\"dicdic\",dictionary)\n",
    "  search_vector = document_to_vector(search_doc, dictionary)\n",
    "  #print(\"search vector:\", search_vector)\n",
    "  similarities = compute_jac_similarity(doc_vectors,search_vector,corpus)\n",
    "  \n",
    "  #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "  similarities = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "  similarities.sort_values([\"Similarity\"], ascending=False)\n",
    "  return similarities\n",
    "\n",
    "\n",
    "\n",
    "####### euclidean distance #################################\n",
    "\n",
    "def compute_euc_similarity(doc_vectors, search_vector, corpus): \n",
    "  similarity_dic = {}\n",
    "    \n",
    "    #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "  for i, doc_vector in enumerate(doc_vectors):\n",
    "      similarity = np.linalg.norm(doc_vector-search_vector)\n",
    "      #print(similarity)\n",
    "      doc_name = corpus[i]\n",
    "      similarity_dic.update({doc_name: similarity})\n",
    "  return similarity_dic\n",
    "\n",
    "def similarity_euc_dist (search_doc,corpus): \n",
    "  dictionary = create_dictionary(corpus)\n",
    "  doc_vectors = np.array([document_to_vector(document, dictionary) for document in corpus])\n",
    "  #print(\"doc vectors: \", doc_vectors)\n",
    "  #print(\"dicdic\",dictionary)\n",
    "  search_vector = document_to_vector(search_doc, dictionary)\n",
    "  #print(\"search vector:\", search_vector)\n",
    "  similarities = compute_euc_similarity(doc_vectors,search_vector,corpus)\n",
    "  \n",
    "  #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "  similarities = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "  similarities.sort_values([\"Similarity\"], ascending=True)\n",
    "  return similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Document  Similarity\n",
      "0      Hello, World!           2\n",
      "1       Bye, World?!           2\n",
      "2  “¶¢[[Hello Mars$%           1\n",
      "            Document  Similarity\n",
      "0      Hello, World!    1.000000\n",
      "1       Bye, World?!    1.000000\n",
      "2  “¶¢[[Hello Mars$%    1.732051\n",
      "            Document  Similarity\n",
      "0      Hello, World!    0.333333\n",
      "1       Bye, World?!    0.333333\n",
      "2  “¶¢[[Hello Mars$%    0.166667\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"Hello, World!\", \"Bye, World?!\", \"“¶¢[[Hello Mars$%\"]\n",
    "\n",
    "search_doc = \"Hello, Hello, World, Bye!\"\n",
    "\n",
    "document_to_vector(search_doc, dictionary)\n",
    "\n",
    "print(similarity_dot_prod(search_doc, corpus))\n",
    "print(similarity_euc_dist(search_doc, corpus))\n",
    "print(similarity_jac_ind(search_doc, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['Hello, world!']\n",
      "['Hello, world!', 'Bye, World!']\n",
      "['Hello, world!', 'Bye, World!', '\"“¶¢[[Hello Mars$%\"']\n",
      "['doc1.txt', 'doc2.txt', 'doc3.txt']\n"
     ]
    }
   ],
   "source": [
    "###load files from folder into the corpus\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "corpus = []\n",
    "word_list = []\n",
    "\n",
    "def read_file(directory,doc_name):\n",
    "    global corpus\n",
    "    global word_list\n",
    "    print(corpus)\n",
    "    \n",
    "    file_to_open = Path(directory+doc_name)\n",
    "    with open(file_to_open, \"r\") as f:\n",
    "       corpus.append(f.read())\n",
    "    word_list.append(doc_name)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "directory = \"/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Foundations of Data Science/Assignment/Final Assignment/\"\n",
    "read_file(directory, \"doc1.txt\")\n",
    "read_file(directory,\"doc2.txt\")\n",
    "read_file(directory,\"doc3.txt\")\n",
    "print(corpus)\n",
    "print(word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bye', 'hello', 'mars', 'marses', 'world'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = set()\n",
    "\n",
    "def process_text(text):\n",
    "  # extract words from string and return in lower case\n",
    "  return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def create_dictionary_1(corpus):\n",
    "  global dictionary\n",
    "  \n",
    "  #iterate through all documents from the corpus\n",
    "  for document in corpus:  \n",
    "    #extract all words in lower case from the string\n",
    "    words = process_text(document)\n",
    "    words_doc = set(words)\n",
    "    #print(words)\n",
    "    dictionary = dictionary.union(words_doc)\n",
    "    \n",
    "    #iterate thorugh the words of each document and append to diciontary when not already in dictionary\n",
    "    ## instead of loop, do set(words) and then union this set with the dictionary set\n",
    "    #for word in words:\n",
    "    #  if word not in dictionary:\n",
    "    #    dictionary.append(word)\n",
    "  #dictionary = sorted(dictionary)\n",
    "  return dictionary \n",
    "\n",
    "\n",
    "corpus = [\"Hello, World!\", \"Bye, World?!\", \"“¶¢[[Hello Mars$%\", \"“¶¢[[Hello Mars$%\", \"“¶¢[[Hello Marses$%\"]\n",
    "\n",
    "create_dictionary_1(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

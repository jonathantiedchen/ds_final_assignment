{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Document Similarity\n",
    "Create a python program that will compute the text document similarity between different docu- ments. Your implementation will take a list of documents as an input text corpus, and it will compute a dictionary of words for the given corpus. Later, when a new document (i.e, search document) is provided, your implementation should provide a list of documents that are similar to the given search document, in descending order of their similarity with the search document.\n",
    "For computing similarity between any two documents in our question, you can use the following distance measures (optionally, you can also use any other measure as well).\n",
    "1. dot product between the two vectors\n",
    "2. distance norm (or Euclidean distance) between two vectors .e.g. || u âˆ’ v ||\n",
    "\n",
    "As part of answering the question, you can also compare and comment on which of the two methods (or any other measure if you have used some other measure) will perform better and what are the reasons for it.\n",
    "\n",
    "Hint A text document can be represented as a word vector against a given dictionary of words. So first, compute the dictionary of words for a given text corpus containing the unique words from the documents of the given corpus. Then transform every text document of the given corpus into vector form, i.e., creating a word vector where 0 indicates the word is not in the document, and 1 indicates that the word is present in the given document. In our question, a text document is just represented as a string, so the text corpus is nothing but a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class DocumentSimilarityAnalyzer:\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.dictionary = set()\n",
    "        self.corpus = [] #list of strings -> appended in the \"update corpus\" function\n",
    "        self.name_list = [] #list of strings -> appended in the \"update name\" function -> used to get document names\n",
    "\n",
    "    def print_dictionary(self): \n",
    "        print(self.dictionary)\n",
    "    \n",
    "    def print_corpus(self):\n",
    "        print(self.corpus)\n",
    "    \n",
    "    def print_name(self): \n",
    "        print(self.name_list)\n",
    "    \n",
    "    def clear_corpus(self):\n",
    "        self.corpus.clear()\n",
    "\n",
    "    def clear_name_list(self):\n",
    "        self.name_list.clear()\n",
    "\n",
    "    def load_search_doc(self, doc_name): #function to load the search document from drive\n",
    "        \n",
    "        #open file by using path() -> to make it operateable on windows and mac systems\n",
    "        file_to_open = Path(self.directory + doc_name)\n",
    "        \n",
    "        #open and read search file\n",
    "        #throw errors in case try fails\n",
    "        try:\n",
    "            with open(file_to_open, \"r\") as f:\n",
    "                search_doc = f.read()\n",
    "            return search_doc\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found - {file_to_open}: {e}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error opening or reading file {file_to_open}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    def update_corpus(self, doc_name): #function to input a new document for the corpus\n",
    "        #open file by using path() -> to make it operateable on windows and mac systems\n",
    "        file_to_open = Path(self.directory + doc_name)\n",
    "        #open and read files to append to the corpus\n",
    "        #throw errors in case try fails\n",
    "        try:\n",
    "            #assert if file is already on name list and therefore part of the corpus\n",
    "            #append file name to name_list when not already on name_list\n",
    "            assert doc_name not in self.name_list, f\"Document {doc_name} already uploaded, please choose another file.\"\n",
    "            self.name_list.append(doc_name)\n",
    "            with open(file_to_open, \"r\") as f:\n",
    "                self.corpus.append(f.read())\n",
    "            self.dictionary = self.create_dictionary()\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found - {file_to_open}: {e}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error opening or reading file {file_to_open}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "        return None # Return a sentinel value or handle the case appropriately\n",
    "\n",
    "    def process_text(self, text): #function to extract words from string\n",
    "        # extract words from string and return in lower case\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def create_dictionary(self): #function to create a dictionary from the corpus documents\n",
    "        \n",
    "        #iterate through all documents from the corpus\n",
    "        for document in self.corpus:\n",
    "            #extract all words in lower case from the string\n",
    "            #save words as a set and add words to the dictionary by building the union\n",
    "            words = self.process_text(document)\n",
    "            new_words = set(words)\n",
    "            self.dictionary = self.dictionary.union(new_words)\n",
    "        return self.dictionary\n",
    "\n",
    "    def document_to_vector(self, document): #function to convert a document (list of document words) into a binary vector\n",
    "        # Convert a document into a binary vector\n",
    "        # uses process_text function to convert document into a list of words\n",
    "        word_list = self.process_text(document)\n",
    "        # iterates through the dictionary  \n",
    "        # appends 1 when word is in the list of words from the document  \n",
    "        # appends 0 when word is not in the list of words from the document  \n",
    "        doc_vector = np.array([1 if word in word_list else 0 for word in self.dictionary])\n",
    "        return doc_vector\n",
    "\n",
    "    def freq_vector(self, document): #Function to convert a document (list of document words) into a frequency vector\n",
    "        #Convert a document into a list of words\n",
    "        word_list = self.process_text(document)\n",
    "        #Count occurrences of each word in the document\n",
    "        word_counts = {word: word_list.count(word) for word in set(word_list)}\n",
    "        #Create the frequency vector\n",
    "        freq_vector = np.array([word_counts.get(word, 0) for word in self.dictionary])\n",
    "        return freq_vector\n",
    "\n",
    "    def dot_similarity(self, search_vector): #function to compute similarities by using dot product\n",
    "        similarity_dic = {}\n",
    "        #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "        for i, doc_vector in enumerate(self.doc_vectors):\n",
    "            similarity = np.dot(doc_vector, search_vector)\n",
    "            doc_name = self.name_list[i]\n",
    "            similarity_dic.update({doc_name: similarity})\n",
    "        return similarity_dic\n",
    "\n",
    "    def jac_similarity(self, search_vector): #function to compute similarities by using Jaccard Index\n",
    "        similarity_dic = {}\n",
    "        for i, doc_vector in enumerate(self.doc_vectors):\n",
    "            cor_len = len(self.process_text(self.corpus[i]))\n",
    "            search_len = len(self.process_text(str(search_vector)))\n",
    "            #divide the dot product by the number of words in the union of search_doc and doc from corpus\n",
    "            similarity = np.dot(doc_vector, search_vector) / (search_len + cor_len)\n",
    "            #get name from name_list by indexing from the name_list\n",
    "            doc_name = self.name_list[i]\n",
    "            similarity_dic.update({doc_name: similarity})\n",
    "        return similarity_dic\n",
    "\n",
    "    def euc_similarity(self, search_vector): #function to compute similarities by using the euclidean distance\n",
    "        similarity_dic = {}\n",
    "        for i, doc_vector in enumerate(self.doc_vectors):\n",
    "            similarity = np.linalg.norm(doc_vector - search_vector)\n",
    "            doc_name = self.name_list[i]\n",
    "            similarity_dic.update({doc_name: similarity})\n",
    "        return similarity_dic\n",
    "\n",
    "    def compute_similarity(self, search_doc, method): #function which is triggered by user to compute similarity\n",
    "        #sends search document string to load search document function and gets the content of the file as string\n",
    "        search_doc = self.load_search_doc(search_doc)\n",
    "        #create biary vector of search document\n",
    "        search_vector = self.document_to_vector(search_doc)\n",
    "        #create array of binary vectors of corpus documents\n",
    "        self.doc_vectors = np.array([self.document_to_vector(document) for document in self.corpus])\n",
    "\n",
    "        #if statements to assess which method is chosen\n",
    "        #sends vectors and corpus to computing function\n",
    "        if method == \"Dot Product\":\n",
    "            similarities = self.dot_similarity(search_vector)\n",
    "            #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "            similarities_df = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "            similarities_df.sort_values([\"Similarity\"], ascending=False, inplace=True)\n",
    "        elif method == \"Jaccard Index\":\n",
    "            similarities = self.jac_similarity(search_vector)\n",
    "            similarities_df = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "            similarities_df.sort_values([\"Similarity\"], ascending=False, inplace=True)\n",
    "        elif method == \"Euclidean Distance\":\n",
    "            similarities = self.euc_similarity(search_vector)\n",
    "            similarities_df = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "            similarities_df.sort_values([\"Similarity\"], ascending=True, inplace=True)\n",
    "        else:\n",
    "            print(\"Unknown method\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"{method}: \\n\", similarities_df)\n",
    "        #return similarities_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first document. It contains some words for testing.', 'The second document is here. It shares some words with the first document.', 'Document number three is different from the others. It has unique words.', 'This is the fourth document. It shares some common words with the other documents.\\n', 'The fifth document contains words that are not present in the search document.\\n']\n",
      "['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt']\n",
      "{'that', 'unique', 'fifth', 'present', 'this', 'second', 'has', 'shares', 'is', 'some', 'testing', 'other', 'common', 'not', 'others', 'three', 'first', 'the', 'fourth', 'words', 'number', 'document', 'are', 'with', 'it', 'contains', 'different', 'from', 'here', 'for', 'documents', 'in', 'search'}\n"
     ]
    }
   ],
   "source": [
    "#replace directory with file path to the folder in which the documents and the code-file are located\n",
    "directory= \"/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Foundations of Data Science/Assignment/Final Assignment/\"\n",
    "analyzer = DocumentSimilarityAnalyzer(directory)\n",
    "\n",
    "analyzer.update_corpus(\"doc1.txt\")\n",
    "analyzer.update_corpus(\"doc2.txt\")\n",
    "analyzer.update_corpus(\"doc3.txt\")\n",
    "analyzer.update_corpus(\"doc4.txt\")\n",
    "analyzer.update_corpus(\"doc5.txt\")\n",
    "analyzer.print_corpus()\n",
    "analyzer.print_name()\n",
    "analyzer.print_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: \n",
      "    Document  Similarity\n",
      "0  doc1.txt           7\n",
      "1  doc2.txt           6\n",
      "3  doc4.txt           6\n",
      "2  doc3.txt           5\n",
      "4  doc5.txt           4\n",
      "Jaccard Index: \n",
      "    Document  Similarity\n",
      "0  doc1.txt    0.159091\n",
      "1  doc2.txt    0.130435\n",
      "3  doc4.txt    0.127660\n",
      "2  doc3.txt    0.111111\n",
      "4  doc5.txt    0.086957\n",
      "Euclidean Distance: \n",
      "    Document  Similarity\n",
      "0  doc1.txt    2.449490\n",
      "1  doc2.txt    2.828427\n",
      "3  doc4.txt    3.162278\n",
      "2  doc3.txt    3.316625\n",
      "4  doc5.txt    3.464102\n"
     ]
    }
   ],
   "source": [
    "analyzer.compute_similarity(\"search_doc.txt\", \"Dot Product\")\n",
    "analyzer.compute_similarity(\"search_doc.txt\", \"Jaccard Index\")\n",
    "analyzer.compute_similarity(\"search_doc.txt\", \"Euclidean Distance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Document Similarity\n",
    "Create a python program that will compute the text document similarity between different docu- ments. Your implementation will take a list of documents as an input text corpus, and it will compute a dictionary of words for the given corpus. Later, when a new document (i.e, search document) is provided, your implementation should provide a list of documents that are similar to the given search document, in descending order of their similarity with the search document.\n",
    "For computing similarity between any two documents in our question, you can use the following distance measures (optionally, you can also use any other measure as well).\n",
    "1. dot product between the two vectors\n",
    "2. distance norm (or Euclidean distance) between two vectors .e.g. || u − v ||\n",
    "\n",
    "As part of answering the question, you can also compare and comment on which of the two methods (or any other measure if you have used some other measure) will perform better and what are the reasons for it.\n",
    "\n",
    "Hint A text document can be represented as a word vector against a given dictionary of words. So first, compute the dictionary of words for a given text corpus containing the unique words from the documents of the given corpus. Then transform every text document of the given corpus into vector form, i.e., creating a word vector where 0 indicates the word is not in the document, and 1 indicates that the word is present in the given document. In our question, a text document is just represented as a string, so the text corpus is nothing but a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "**def_input_corpus**: \n",
    "- input text corpus containing of multiple strings CHECK \n",
    "- words from corpus to dictionary CHECK\n",
    "- split inpt strings into words (account for .:,;!&/()=\"\")) CHECK\n",
    "- append words to a library (list of distinct words) CHECK\n",
    "- create vektors fo text documents CHECK\n",
    "\n",
    "\n",
    "\n",
    "**def_input_new_document**:\n",
    "- create vektor for text document based on corpus CHECK\n",
    "- calculate difference between input text and every existing text document CHECK\n",
    "- create a list of the documents and their similarity rating -> dictionary used -> probably convert it to df in the end\n",
    "\n",
    "\n",
    "**things to add**:\n",
    "- document names CHECK\n",
    "- reading of documents from drive path (.txt file) CHECK -> test with windows \n",
    "- word frequency for documents -> not binary vector anymore -> does that make sense?\n",
    "- change dictionary to set instead of list CHECK\n",
    "- combine the similarity methods and call them individually be inputting \"method = ...\" -> similar to ChatGPT Code\n",
    "\n",
    "Error Handling: \n",
    "- Error when document is already part of the corpus (asserted by checking if already in name list) CHECK\n",
    "- Error when search document name and document name for corpus are not in the folder CHECK\n",
    "- Error when file is empty (when contains no words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Jonathan \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "import sys\n",
    "\n",
    "#global variables used accross all functions\n",
    "dictionary = set()\n",
    "corpus = [] #list of strings -> appended in the \"update corpus\" function\n",
    "name_list = [] #list of strings -> appended in the \"update name\" function -> used to get document names\n",
    "\n",
    "def clear_corpus (corpus):\n",
    "  corpus = corpus.clear()\n",
    "  return corpus\n",
    "\n",
    "def clear_name_list(name_list):\n",
    "  name_list = name_list.clear()\n",
    "  return name_list\n",
    "\n",
    "def set_directory(input_directory): #sets the input directory as a global variable so is is usable accross all functions\n",
    "  global directory \n",
    "  directory = input_directory\n",
    "  return directory\n",
    "\n",
    "def load_search_doc(doc_name): #function to load the search document from drive\n",
    "  file_to_open = Path(directory+doc_name)\n",
    "  try:  \n",
    "    with open(file_to_open, \"r\") as f:\n",
    "      search_doc = f.read()\n",
    "    return search_doc\n",
    "  except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_to_open}\")\n",
    "  except IOError as e:\n",
    "        print(f\"Error opening or reading file {file_to_open}: {e}\")\n",
    "  except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "  return None  # Return a sentinel value or handle the case appropriately\n",
    "\n",
    "def update_corpus(directory,doc_name): #function to input a new document for the corpus\n",
    "  global corpus\n",
    "  global name_list\n",
    "    \n",
    "  #open file by using path() -> to make it operateable on windows and mac systems\n",
    "  file_to_open = Path(directory+doc_name)\n",
    "    \n",
    "  #open and read file to append the content to the corpus\n",
    "  \n",
    "  assert doc_name not in name_list, f\"Document {doc_name} already read uploaded, please choose another file.\"\n",
    "  name_list.append(doc_name)\n",
    "  \n",
    "  #open and read file to append the content to the corpus\n",
    "  try:\n",
    "    with open(file_to_open, \"r\") as f:\n",
    "      corpus.append(f.read())\n",
    "  #append file name to name_list\n",
    "    dictionary = create_dictionary(corpus)\n",
    "    #return corpus, dictionary, name_list\n",
    "  except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_to_open}\")\n",
    "  except IOError as e:\n",
    "        print(f\"Error opening or reading file {file_to_open}: {e}\")\n",
    "  except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "  return None  # Return a sentinel value or handle the case appropriately\n",
    "\n",
    "def process_text(text): #function to extract words from string\n",
    "  # extract words from string and return in lower case\n",
    "  return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def create_dictionary(corpus): #function to create a dictionary from the corpus documents\n",
    "  global dictionary\n",
    "  #iterate through all documents from the corpus\n",
    "  for document in corpus:  \n",
    "    #extract all words in lower case from the string\n",
    "    #save words as a set and add words to the dictionary by building the union\n",
    "    words = process_text(document)\n",
    "    words_doc = set(words)\n",
    "    dictionary = dictionary.union(words_doc)\n",
    "  \n",
    "  return dictionary \n",
    "\n",
    "def document_to_vector(document, dictionary): #function to convert a document (list of document words) into a binary vector\n",
    "  # Convert a document into a binary vector\n",
    "  # uses process_text function to convert document into a list of words\n",
    "  word_list = process_text(document)\n",
    "  # iterates through the dictionary  \n",
    "  # appends 1 when word is in the list of words from the document  \n",
    "  # appends 0 when word is not in the list of words from the document  \n",
    "  doc_vector = [1 if word in word_list else 0 for word in dictionary]\n",
    "  return np.array(doc_vector)\n",
    "\n",
    "def freq_vector(document, dictionary):# Function to convert a document (list of document words) into a frequency vector\n",
    "    \n",
    "    # Convert a document into a list of words\n",
    "    word_list = process_text(document)\n",
    "    \n",
    "    # Count occurrences of each word in the document\n",
    "    word_counts = {word: word_list.count(word) for word in set(word_list)}\n",
    "    \n",
    "    # Create the frequency vector\n",
    "    doc_vector = [word_counts.get(word, 0) for word in dictionary]\n",
    "    \n",
    "    # Convert the list to a NumPy array\n",
    "    return np.array(doc_vector)\n",
    "\n",
    "\n",
    "\n",
    "def dot_similarity(doc_vectors, search_vector, corpus): #function to compute similarities by using dot product\n",
    "    similarity_dic = {}\n",
    "    #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "    for i, doc_vector in enumerate(doc_vectors):\n",
    "        similarity = np.dot(doc_vector, search_vector)\n",
    "        #similarity=np.dot(doc, search_vector)/len(process_text(corpus[i]))\n",
    "        doc_name = name_list[i]\n",
    "        similarity_dic.update({doc_name: similarity})\n",
    "    return similarity_dic\n",
    "\n",
    "\n",
    "def jac_similarity(doc_vectors, search_vector, corpus): #function to compute similarities by using Jaccard Index\n",
    "    similarity_dic = {}\n",
    "    #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "    for i, doc_vector in enumerate(doc_vectors):\n",
    "        cor_len= len(process_text(corpus[i]))\n",
    "        search_len = len(process_text(str(search_vector)))\n",
    "        \n",
    "        #divide the dot product by the number of words in the union of search_doc and doc from corpus\n",
    "        similarity=np.dot(doc_vector, search_vector)/(search_len+cor_len)\n",
    "\n",
    "        #get name from name_list by indexing from the name_list\n",
    "        doc_name = name_list[i]\n",
    "        similarity_dic.update({doc_name: similarity})\n",
    "    \n",
    "    return similarity_dic\n",
    "\n",
    "\n",
    "def euc_similarity(doc_vectors, search_vector, corpus): #function to compute similarities by using the euclidean distance\n",
    "  similarity_dic = {}\n",
    "    \n",
    "    #use \"for i, doc_vector\" to also get index of the iteration -> used to get the document from the corpus\n",
    "  for i, doc_vector in enumerate(doc_vectors):\n",
    "      similarity = np.linalg.norm(doc_vector-search_vector)\n",
    "      #print(similarity)\n",
    "      doc_name = name_list[i]\n",
    "      similarity_dic.update({doc_name: similarity})\n",
    "  return similarity_dic\n",
    "  \n",
    "  \n",
    "\n",
    "def compute_similarity(search_doc, corpus, method):\n",
    "    \n",
    "    #global dictionary\n",
    "    \n",
    "    #sends search document string to load search document function and gets the content of the file as string\n",
    "    search_doc = load_search_doc(search_doc)\n",
    "    \n",
    "    #create biary vector of search document\n",
    "    search_vector = document_to_vector(search_doc, dictionary)\n",
    "    \n",
    "    #create array of binary vectors of corpus documents\n",
    "    doc_vectors = np.array([document_to_vector(document, dictionary) for document in corpus])\n",
    "\n",
    "  #if statements to assess which method is chosen\n",
    "  #sends vectors and corpus to computing function\n",
    "  #prints similirities\n",
    "    if method == \"Dot Product\": \n",
    "      similarities = dot_similarity(doc_vectors,search_vector,corpus)\n",
    "      #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "      similarities = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "      similarities.sort_values([\"Similarity\"], ascending=False)\n",
    "      print(\"Dot Product: \\n\",similarities)\n",
    "    elif method == \"Jaccard Index\": \n",
    "      similarities = jac_similarity(doc_vectors,search_vector,corpus)\n",
    "      #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "      similarities = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "      similarities.sort_values([\"Similarity\"], ascending=False)\n",
    "      print(\"Jaccard Index: \\n\",similarities)\n",
    "    elif method == \"Eucledian Distance\": \n",
    "      similarities = euc_similarity(doc_vectors,search_vector,corpus)\n",
    "      #convert dictionary into data frame for formatted output and possibility to easily order results\n",
    "      similarities = pd.DataFrame(similarities.items(), columns=['Document', 'Similarity'])\n",
    "      similarities.sort_values([\"Similarity\"], ascending=False)\n",
    "      print(\"Eucledian Distance: \\n\",similarities)\n",
    "    else: print(\"Unknown method\")\n",
    "\n",
    "    #return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Document doc1.txt already read uploaded, please choose another file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#replace directory with file path to the folder in which the documents and the code-file are located\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m set_directory(\u001b[39m\"\u001b[39m\u001b[39m/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Foundations of Data Science/Assignment/Final Assignment/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=4'>5</a>\u001b[0m update_corpus(directory, \u001b[39m\"\u001b[39;49m\u001b[39mdoc1.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=5'>6</a>\u001b[0m update_corpus(directory,\u001b[39m\"\u001b[39m\u001b[39mdoc2.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=6'>7</a>\u001b[0m update_corpus(directory,\u001b[39m\"\u001b[39m\u001b[39mdoc3.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=41'>42</a>\u001b[0m file_to_open \u001b[39m=\u001b[39m Path(directory\u001b[39m+\u001b[39mdoc_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#open and read file to append the content to the corpus\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=45'>46</a>\u001b[0m \u001b[39massert\u001b[39;00m doc_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m name_list, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDocument \u001b[39m\u001b[39m{\u001b[39;00mdoc_name\u001b[39m}\u001b[39;00m\u001b[39m already read uploaded, please choose another file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=46'>47</a>\u001b[0m name_list\u001b[39m.\u001b[39mappend(doc_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://github/jonathantiedchen/ds_final_assignment/Final_Assignment_Q3.ipynb#X23sdnNjb2RlLXZmcw%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#open and read file to append the content to the corpus\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Document doc1.txt already read uploaded, please choose another file."
     ]
    }
   ],
   "source": [
    "#replace directory with file path to the folder in which the documents and the code-file are located\n",
    "set_directory(\"/Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Foundations of Data Science/Assignment/Final Assignment/\")\n",
    "\n",
    "\n",
    "update_corpus(directory, \"doc1.txt\")\n",
    "update_corpus(directory,\"doc2.txt\")\n",
    "update_corpus(directory,\"doc3.txt\")\n",
    "update_corpus(directory,\"doc4.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, world!', 'Bye, World!', 'Das ist Mars Mars Textdokument. Cool', 'Das ist das nächste Testdokument. Mars!']\n",
      "Hello, Hello, World, Bye!\n",
      "['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt']\n",
      "Dot Product: \n",
      "    Document  Similarity\n",
      "0  doc1.txt           2\n",
      "1  doc2.txt           1\n",
      "2  doc3.txt           3\n",
      "3  doc4.txt           5\n",
      "Jaccard Index: \n",
      "    Document  Similarity\n",
      "0  doc1.txt    0.166667\n",
      "1  doc2.txt    0.083333\n",
      "2  doc3.txt    0.187500\n",
      "3  doc4.txt    0.312500\n",
      "Eucledian Distance: \n",
      "    Document  Similarity\n",
      "0  doc1.txt    2.236068\n",
      "1  doc2.txt    2.645751\n",
      "2  doc3.txt    2.449490\n",
      "3  doc4.txt    1.414214\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "print(search_doc)\n",
    "print(name_list)\n",
    "\n",
    "compute_similarity(\"search_doc.txt\", corpus, \"Dot Product\")\n",
    "compute_similarity(\"search_doc.txt\", corpus, \"Jaccard Index\")\n",
    "compute_similarity(\"search_doc.txt\", corpus, \"Eucledian Distance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_name_list(name_list)\n",
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_corpus(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found - /Users/jonathan/Library/Mobile Documents/com~apple~CloudDocs/Master/Foundations of Data Science/Assignment/Final Assignment/doc1s.txt\n"
     ]
    }
   ],
   "source": [
    "update_corpus(directory, \"doc1s.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
